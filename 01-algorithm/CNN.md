- [输入层（Input Layer）](#输入层input-layer)
- [卷积层（Convolutional Layer）](#卷积层convolutional-layer)
- [激活函数（Activation Function）](#激活函数activation-function)
  - [基础激活函数](#基础激活函数)
    - [Sigmoid](#sigmoid)
    - [Tanh（双曲正切）](#tanh双曲正切)
    - [ReLU（Rectified Linear Unit）](#relurectified-linear-unit)
  - [改进型激活函数](#改进型激活函数)
    - [Leaky ReLU](#leaky-relu)
    - [Parametric ReLU (PReLU)](#parametric-relu-prelu)
    - [ELU（Exponential Linear Unit）](#eluexponential-linear-unit)
    - [Swish](#swish)
- [池化层（Pooling Layer）](#池化层pooling-layer)
  - [池化层的基本原理](#池化层的基本原理)
  - [池化层的主要类型](#池化层的主要类型)
    - [最大池化（Max Pooling）：](#最大池化max-pooling)
    - [平均池化（Average Pooling）：](#平均池化average-pooling)
    - [其他变体：](#其他变体)
  - [池化层的作用](#池化层的作用)
  - [池化层的特性](#池化层的特性)
  - [池化层的局限性及替代方案](#池化层的局限性及替代方案)
    - [局限性：](#局限性)
    - [替代方案：](#替代方案)
  - [经典网络中的池化层应用](#经典网络中的池化层应用)
    - [LeNet-5：](#lenet-5)
    - [AlexNet：](#alexnet)
    - [VGGNet：](#vggnet)
    - [现代网络趋势：](#现代网络趋势)
- [全连接层（Fully Connected Layer）](#全连接层fully-connected-layer)
  - [全连接层的基本原理](#全连接层的基本原理)
    - [结构特点：](#结构特点)
    - [功能核心：](#功能核心)
    - [计算过程：](#计算过程)
  - [全连接层的作用](#全连接层的作用)
    - [特征整合：](#特征整合)
    - [非线性映射：](#非线性映射)
    - [输出适配：](#输出适配)
  - [全连接层与卷积层的区别](#全连接层与卷积层的区别)
  - [全连接层的问题与改进](#全连接层的问题与改进)
    - [参数量过大：](#参数量过大)
    - [空间信息丢失：](#空间信息丢失)
    - [替代方案：](#替代方案-1)
- [输出层（Output Layer）](#输出层output-layer)
- [其他辅助组件](#其他辅助组件)



CNN 的核心架构通过 “卷积-激活-池化” 的交替堆叠，逐层提取从低级到高级的特征，最终通过全连接层输出结果。其设计巧妙结合了局部感知、权值共享、平移不变性等特性，使其成为计算机视觉领域的基石模型。

卷积神经网络（Convolutional Neural Network, CNN）是深度学习中用于处理图像、视频、语音等具有空间或时序结构数据的核心模型。其核心架构由以下几个关键组件构成：





# 输入层（Input Layer）

功能：接收原始数据（如图像的像素矩阵）。

特点：需标准化（如归一化到 [0,1] 或 [-1,1]）以加速训练。

示例：输入图像尺寸为 28×28×3（RGB三通道）。




# 卷积层（Convolutional Layer）

功能：提取局部特征（如边缘、纹理等）。

核心操作：

卷积核（Filter）：滑动窗口在输入数据上计算局部区域的加权和。

特征图（Feature Map）：每个卷积核生成一个特征图，捕捉特定模式。

关键参数：

卷积核尺寸（如 3×3、5×5）。

步长（Stride）：滑动步长（如 1 或 2）。

填充（Padding）：边缘补零（same 保持尺寸，valid 不填充）。

数学形式：

$$
output(i,j)=\sum_{m}^{}{\sum_{n}^{}{input(i+m,j+n)⋅kernel(m,n)+offset}}
$$


[卷积的基本原理请查考另一篇文档](https://github.com/laneston/note/blob/main/01-algorithm/Convolution.md)


# 激活函数（Activation Function）

激活函数的选择直接影响模型性能。ReLU及其变体（如Leaky ReLU、Swish）是现代深度学习的首选，而Sigmoid/Softmax 专用于输出层。理解其数学特性及适用场景，结合任务需求调整，是优化模型的关键步骤。


功能：引入非线性，增强模型表达能力。

常用类型：

ReLU（Rectified Linear Unit）：f(x) = max(0, x)（缓解梯度消失，计算高效）。

Sigmoid、Tanh（较少用于中间层，多用于输出层）。

Leaky ReLU、Swish（改进非线性特性）。


## 基础激活函数

### Sigmoid

$$
σ(x)=\frac{1}{1+{{e}^{-x}}}
$$


**特点：**

输出范围 (0,1)，适合二分类输出层。

梯度饱和问题：输入较大或较小时梯度接近零，导致梯度消失。

**使用场景：**

二分类输出层（如逻辑回归）。

传统神经网络（现多被ReLU替代）。

**PyTorch函数：**

torch.nn.Sigmoid()

**MATLAB**

```
% 生成x值范围
x = linspace(-5, 5, 100);

% 定义Sigmoid函数
sigmoid = @(x) 1 ./ (1 + exp(-x));

% 计算对应的y值
y = sigmoid(x);

% 绘制图像
figure;
plot(x, y, 'b', 'LineWidth', 2); % 蓝色粗线
hold on;
plot(0, 0.5, 'ro', 'MarkerSize', 8, 'MarkerFaceColor', 'r'); % 标记中心点
title('Sigmoid函数');
xlabel('x');
ylabel('f(x)');
grid on;
axis([-5 5 0 1]); % 设置坐标轴范围
hold off;
```


### Tanh（双曲正切）

$$
tanh(x)=\frac{{{e}^{x}}-{{e}^{-x}}}{{{e}^{x}}+{{e}^{-x}}}
$$


特点：

输出范围 (-1, 1)，均值为零，收敛速度比Sigmoid快。

同样存在梯度饱和问题。

使用场景：

循环神经网络（RNN）的隐藏层。

需零中心化输出的场景。

**PyTorch函数：**

torch.nn.Tanh()




### ReLU（Rectified Linear Unit）

$$
ReLU(x)=max(0,x)
$$

特点：

计算高效，缓解梯度消失（正区间梯度为1）。

输出非零中心化，存在“死亡ReLU”问题（负数输入梯度为零）。

使用场景：

卷积神经网络（CNN）和全连接层的隐藏层（默认选择）。

**PyTorch函数：**

torch.nn.ReLU()


## 改进型激活函数


### Leaky ReLU


$$
LeakyReLU(x)=\left \{{\begin{matrix}x&if x>0\\αx&otherwise\end{matrix}}\right .
$$

（默认 $α=0.01$）

特点：

解决“死亡ReLU”问题，允许负数区间有微小梯度。

使用场景：

需缓解神经元死亡问题的深层网络。


**PyTorch函数：**

torch.nn.LeakyReLU(negative_slope=0.01)

**MATLAB**



### Parametric ReLU (PReLU)

特点：

类似Leaky ReLU，但 α 是可学习参数。

使用场景：

需要自适应调整负数区间的场景。

**PyTorch函数：**

torch.nn.PReLU(num_parameters=1, init=0.25)

**MATLAB**


### ELU（Exponential Linear Unit）

$$
ELU(x)\left \{{\begin{matrix}x&if x>0\\α({{e}^{x}}-1)&otherwise\end{matrix}}\right .
$$

特点：

负数区间输出平滑，接近零均值，缓解梯度消失。

计算复杂度略高（涉及指数运算）。

使用场景：

对噪声敏感的任务（如图像生成）。

**PyTorch函数：**

torch.nn.ELU(alpha=1.0)

**MATLAB**

### Swish

$$
Swish(x)=x⋅σ(βx)
$$

（β 可学习，默认 β=1）

特点：

Google提出，平滑非单调，实验表明在深层网络中优于ReLU。

使用场景：

替代ReLU用于复杂模型（如Transformer、ResNet）。

**PyTorch函数：**

```
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)
```











# 池化层（Pooling Layer）

池化层通过下采样和特征压缩，使CNN更高效、鲁棒，但其固定规则和信息丢失问题也推动了替代方案的发展。理解池化层的原理有助于设计网络时权衡分辨率、计算量和模型性能。

池化层（Pooling Layer）是卷积神经网络（CNN）中的核心组件之一，主要用于降低特征图的空间维度、减少计算量、增强模型的鲁棒性（如平移不变性），同时保留关键特征。其核心原理和设计思想如下：


## 池化层的基本原理

操作定义：
池化层对输入的特征图（Feature Map）进行局部区域的下采样，即在一个滑动窗口（如 2×2 或 3×3）内，通过某种规则（如取最大值或平均值）提取该区域的代表值，生成更小的特征图。

关键参数：

池化窗口大小（Pooling Kernel Size）：如 2×2、3×3。

步长（Stride）：窗口滑动的步长，通常与窗口大小相同（如步长=2，窗口=2×2），避免重叠。

填充（Padding）：通常为“有效填充”（Valid Padding，即不填充），直接舍弃边界无法覆盖的区域。

计算过程示例（以最大池化为例）：

输入特征图尺寸：4×4

池化窗口：2×2，步长=2

输出特征图尺寸：2×2

每个窗口取最大值：

```
输入区域 [[1, 3],     → 输出值 3  
       [2, 4]]  
```

## 池化层的主要类型

### 最大池化（Max Pooling）：

原理：取窗口内的最大值作为输出。

优点：保留最显著的特征（如边缘、纹理），增强模型对微小平移的鲁棒性。

适用场景：图像分类、物体检测等需要突出关键特征的任务。

### 平均池化（Average Pooling）：

原理：取窗口内的平均值作为输出。

优点：平滑特征，抑制噪声。

适用场景：对局部细节不敏感的任务（如信号处理）。

### 其他变体：

L2池化：计算窗口内值的平方和的平方根。

重叠池化（Overlapping Pooling）：步长 < 窗口大小（如窗口=3×3，步长=2）。

全局池化（Global Pooling）：将整个特征图压缩为单个值（常用于网络末端替代全连接层）。

## 池化层的作用

降低计算复杂度：

通过下采样减少特征图的尺寸（如从 224×224 降至 112×112），大幅减少后续层的计算量和参数量。

平移不变性（Translation Invariance）：

池化操作对输入特征的微小位移不敏感。例如，无论目标在窗口内如何轻微移动，最大池化仍能提取到关键特征。

防止过拟合：

通过降维减少模型参数，抑制噪声干扰，增强泛化能力。

扩大感受野：

随着网络加深，池化层逐步扩大后续卷积层的感受野，使高层特征能覆盖更广的输入区域。

## 池化层的特性


| 特性       | 说明                                                           |
| :--------- | :------------------------------------------------------------- |
| 无参数学习 | 池化层没有可训练的权重参数，仅通过固定规则计算。               |
| 局部连接   | 仅处理局部区域，类似卷积操作，但不可学习。                     |
| 通道独立   | 池化操作在每个通道上独立进行，输出通道数与输入相同。           |
| 不可逆操作 | 池化是信息丢失的过程，无法通过反向传播恢复细节（与卷积不同）。 |


## 池化层的局限性及替代方案

### 局限性：

信息丢失：下采样可能丢失细粒度特征（如精确定位信息），不适用于密集预测任务（如语义分割）。

固定规则：池化规则（如最大/平均）是人为设定，可能无法自适应数据特性。

### 替代方案：

步长卷积（Strided Convolution）：用步长>1的卷积层替代池化层（如ResNet），实现可学习的下采样。

空洞卷积（Dilated Convolution）：扩大感受野而不降低分辨率。

空间金字塔池化（SPP）：生成多尺度特征，避免输入尺寸固定。


## 经典网络中的池化层应用

### LeNet-5：

使用 2×2 平均池化，步长=2，逐步压缩特征图尺寸。

### AlexNet：

采用 3×3 最大池化，步长=2，增强平移不变性。

### VGGNet：

通过多个 2×2 最大池化层（步长=2）逐级降维。

### 现代网络趋势：

部分网络（如ResNet）减少池化层，改用步长卷积；全卷积网络（FCN）完全摒弃池化，保留高分辨率特征。





















# 全连接层（Fully Connected Layer）

全连接层的核心是通过全局连接整合特征，完成从特征到目标的映射。尽管参数量大且可能丢失空间信息，但它仍是许多经典 CNN 的关键组件。现代网络（如 ResNet、Transformer）通过改进设计（如 GAP、自注意力）逐渐减少对全连接层的依赖，但在特定任务中（如分类头）仍不可或缺。

全连接层（Fully Connected Layer，FC Layer）是卷积神经网络（CNN）中的关键组成部分，通常位于网络的末端，负责将卷积层和池化层提取的局部特征整合为全局特征，并完成最终的分类或回归任务。其原理和特点如下：

## 全连接层的基本原理

### 结构特点：

全连接层的每个神经元与前一层的所有神经元相连。例如，若前一层的输出是 N 个特征（如经过展平后的特征向量），全连接层有 M 个神经元，则权重矩阵的维度为 N×M，加上偏置项 M，总参数量为 N×M + M。

### 功能核心：

通过线性变换（矩阵乘法）和非线性激活函数（如 ReLU、Softmax），全连接层将输入特征映射到目标任务的输出空间（如分类类别）。例如：

分类任务：最后一层全连接层的输出维度等于类别数，后接 Softmax 函数得到概率分布。

回归任务：直接输出预测值（如坐标、数值）。

### 计算过程：

输入数据（展平后的特征向量）与权重矩阵相乘，加上偏置，再通过激活函数：$y=σ(W⋅x+b)$

其中，x 是输入特征，W 是权重矩阵，b 是偏置，σ 是激活函数。

## 全连接层的作用

### 特征整合：

卷积层提取的是局部特征（如边缘、纹理），全连接层通过全局连接整合这些特征，学习它们之间的高阶关系（如物体整体结构）。

### 非线性映射：

通过激活函数（如 ReLU），全连接层能增强网络的非线性表达能力，解决复杂任务。

### 输出适配：

将高维特征映射到任务所需的低维输出空间（如分类标签）。

## 全连接层与卷积层的区别


| 特性     | 全连接层                           | 卷积层                         |
| :------- | :--------------------------------- | :----------------------------- |
| 连接方式 | 每个神经元与前一层的所有神经元连接 | 局部连接（感受野）             |
| 参数共享 | 无参数共享                         | 权值共享（减少参数量）         |
| 空间信息 | 丢失空间结构（输入需展平）         | 保留空间结构（处理多维特征图） |
| 参数量   | 巨大（易导致过拟合）               | 较少（高效提取局部特征）       |


## 全连接层的问题与改进

### 参数量过大：

全连接层的参数量通常占整个 CNN 的 80% 以上，容易导致过拟合。
解决方案：

使用 Dropout 随机丢弃神经元（如 AlexNet）。

用全局平均池化（Global Average Pooling，GAP）替代全连接层（如 ResNet）。

采用 1×1 卷积降维（如 NiN）。

### 空间信息丢失：

输入特征需展平为一维向量，丢失了空间信息。

### 替代方案：

全卷积网络（FCN）直接输出空间预测结果（如语义分割）。




















# 输出层（Output Layer）

功能：根据任务类型生成最终结果。

常见形式：

分类任务：Softmax 函数输出概率分布（如 [0.1, 0.7, 0.2]）。

回归任务：线性输出（如预测坐标值、价格等）。













# 其他辅助组件

批量归一化（Batch Normalization）：

加速训练，缓解梯度消失/爆炸，减少对初始化的敏感度。

Dropout：

随机丢弃部分神经元，防止过拟合。

残差连接（Residual Connection）：

解决深层网络梯度退化问题（如 ResNet 中的跳跃连接）。




**以经典的 LeNet-5 为例：**

```
输入层 → 卷积层 → 激活函数（ReLU） → 池化层 → 卷积层 → 激活函数 → 池化层 → 全连接层 → 输出层（Softmax）
```




